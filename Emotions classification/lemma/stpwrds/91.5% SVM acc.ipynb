{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1085454,"sourceType":"datasetVersion","datasetId":605165}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ntrain_root = \"/kaggle/input/emotions-dataset-for-nlp/train.txt\"\nval_root = \"/kaggle/input/emotions-dataset-for-nlp/val.txt\"\ntest_root = \"/kaggle/input/emotions-dataset-for-nlp/test.txt\"\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-14T19:29:40.392991Z","iopub.execute_input":"2024-07-14T19:29:40.393794Z","iopub.status.idle":"2024-07-14T19:29:40.400549Z","shell.execute_reply.started":"2024-07-14T19:29:40.393742Z","shell.execute_reply":"2024-07-14T19:29:40.399242Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_root, delimiter=\";\")\ntrain_df.columns = [\"text\", \"emotion\"]\nval_df = pd.read_csv(val_root, delimiter=\";\")\nval_df.columns = [\"text\", \"emotion\"]\ntest_df = pd.read_csv(test_root, delimiter=\";\")\ntest_df.columns = [\"text\", \"emotion\"]","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:29:40.507431Z","iopub.execute_input":"2024-07-14T19:29:40.507900Z","iopub.status.idle":"2024-07-14T19:29:40.566495Z","shell.execute_reply.started":"2024-07-14T19:29:40.507840Z","shell.execute_reply":"2024-07-14T19:29:40.565239Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the class column\ntrain_df['emotion'] = label_encoder.fit_transform(train_df['emotion'])\nval_df['emotion'] = label_encoder.fit_transform(val_df['emotion'])\ntest_df['emotion'] = label_encoder.fit_transform(test_df['emotion'])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:29:40.701001Z","iopub.execute_input":"2024-07-14T19:29:40.701434Z","iopub.status.idle":"2024-07-14T19:29:40.718114Z","shell.execute_reply.started":"2024-07-14T19:29:40.701401Z","shell.execute_reply":"2024-07-14T19:29:40.716885Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:04:59.502597Z","iopub.execute_input":"2024-07-14T19:04:59.503311Z","iopub.status.idle":"2024-07-14T19:04:59.521366Z","shell.execute_reply.started":"2024-07-14T19:04:59.503268Z","shell.execute_reply":"2024-07-14T19:04:59.520201Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text  emotion\n0  i can go from feeling so hopeless to so damned...        4\n1   im grabbing a minute to post i feel greedy wrong        0\n2  i am ever feeling nostalgic about the fireplac...        3\n3                               i am feeling grouchy        0\n4  ive been feeling a little burdened lately wasn...        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i can go from feeling so hopeless to so damned...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>im grabbing a minute to post i feel greedy wrong</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i am ever feeling nostalgic about the fireplac...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i am feeling grouchy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ive been feeling a little burdened lately wasn...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:04:59.523159Z","iopub.execute_input":"2024-07-14T19:04:59.523514Z","iopub.status.idle":"2024-07-14T19:05:00.849647Z","shell.execute_reply.started":"2024-07-14T19:04:59.523487Z","shell.execute_reply":"2024-07-14T19:05:00.848364Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:05:00.853154Z","iopub.execute_input":"2024-07-14T19:05:00.853872Z","iopub.status.idle":"2024-07-14T19:05:02.443441Z","shell.execute_reply.started":"2024-07-14T19:05:00.853790Z","shell.execute_reply":"2024-07-14T19:05:02.442058Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:05:02.445633Z","iopub.execute_input":"2024-07-14T19:05:02.446053Z","iopub.status.idle":"2024-07-14T19:05:02.453927Z","shell.execute_reply.started":"2024-07-14T19:05:02.446018Z","shell.execute_reply":"2024-07-14T19:05:02.452262Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    tokenized = word_tokenize(text)\n    \n    lemmatizer = WordNetLemmatizer()\n\n    stop_words = set(stopwords.words('english'))\n    \n    lemmatized = [lemmatizer.lemmatize(word.lower()) for word in tokenized if word.lower() not in stop_words]\n    \n    return \" \".join(lemmatized)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:05:02.455964Z","iopub.execute_input":"2024-07-14T19:05:02.456540Z","iopub.status.idle":"2024-07-14T19:05:02.469744Z","shell.execute_reply.started":"2024-07-14T19:05:02.456503Z","shell.execute_reply":"2024-07-14T19:05:02.468587Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_df[\"text\"] = train_df[\"text\"].apply(preprocess_text)\nval_df[\"text\"] = val_df[\"text\"].apply(preprocess_text)\ntest_df[\"text\"] = test_df[\"text\"].apply(preprocess_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:05:02.471535Z","iopub.execute_input":"2024-07-14T19:05:02.472115Z","iopub.status.idle":"2024-07-14T19:05:16.430344Z","shell.execute_reply.started":"2024-07-14T19:05:02.472080Z","shell.execute_reply":"2024-07-14T19:05:16.428708Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:05:16.431843Z","iopub.execute_input":"2024-07-14T19:05:16.432229Z","iopub.status.idle":"2024-07-14T19:05:16.445302Z","shell.execute_reply.started":"2024-07-14T19:05:16.432195Z","shell.execute_reply":"2024-07-14T19:05:16.444008Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                text  emotion\n0  go feeling hopeless damned hopeful around some...        4\n1          im grabbing minute post feel greedy wrong        0\n2  ever feeling nostalgic fireplace know still pr...        3\n3                                    feeling grouchy        0\n4      ive feeling little burdened lately wasnt sure        4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>go feeling hopeless damned hopeful around some...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>im grabbing minute post feel greedy wrong</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ever feeling nostalgic fireplace know still pr...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>feeling grouchy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ive feeling little burdened lately wasnt sure</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df[\"emotion\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:05:16.446951Z","iopub.execute_input":"2024-07-14T19:05:16.447712Z","iopub.status.idle":"2024-07-14T19:05:16.473459Z","shell.execute_reply.started":"2024-07-14T19:05:16.447667Z","shell.execute_reply":"2024-07-14T19:05:16.472083Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"emotion\n2    5362\n4    4665\n0    2159\n1    1937\n3    1304\n5     572\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"X_train = train_df[\"text\"]\nX_val = val_df[\"text\"]\nX_test = test_df[\"text\"]\n\ny_train = train_df[\"emotion\"]\ny_val = val_df[\"emotion\"]\ny_test = test_df[\"emotion\"]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:29:46.959918Z","iopub.execute_input":"2024-07-14T19:29:46.960323Z","iopub.status.idle":"2024-07-14T19:29:46.970731Z","shell.execute_reply.started":"2024-07-14T19:29:46.960292Z","shell.execute_reply":"2024-07-14T19:29:46.969060Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n# Initialize the vectorizer and SVD model\nvectorizer = TfidfVectorizer(ngram_range=(1, 1))\n#svd = TruncatedSVD(n_components=1000)  # Adjust n_components as needed\n\n# Fit and transform the training data\nX_train_vec = vectorizer.fit_transform(X_train)\n# X_train_svd = svd.fit_transform(X_train_vec)\n\n# Transform the validation and test data\nX_val_vec = vectorizer.transform(X_val)\n# X_val_svd = svd.transform(X_val_vec)\n\nX_test_vec = vectorizer.transform(X_test)\n# X_test_svd = svd.transform(X_test_vec)\n\nprint(f\"Original number of features: {X_train_vec.shape[1]}\")\n# print(f\"Reduced number of features: {X_train_svd.shape[1]}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T17:55:16.150458Z","iopub.execute_input":"2024-07-14T17:55:16.150922Z","iopub.status.idle":"2024-07-14T17:55:16.570867Z","shell.execute_reply.started":"2024-07-14T17:55:16.150889Z","shell.execute_reply":"2024-07-14T17:55:16.569427Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Original number of features: 13456\n","output_type":"stream"}]},{"cell_type":"code","source":"# grid_search = GridSearchCV(model, {\n#     \"C\": [0.8, 0.9, 1.0], \n#     \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n#     \"degree\": [3, 4, 5, 6, 7, 8]\n# }, verbose=3)\n\n# grid_search.fit(X_train_vec, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SVC(kernel=\"linear\")\nmodel.fit(X_train_vec, y_train)\npreds = model.predict(X_test_vec)\n\nscore = accuracy_score(preds, y_test)\nscore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SVC(kernel=\"linear\", class_weight=\"balanced\")\n# 2    5362\n# 4    4665\n# 0    2159\n# 1    1937\n# 3    1304\n# 5     572\nmodel.fit(X_train_vec, y_train)\npreds = model.predict(X_test_vec)\nscore = accuracy_score(preds, y_test)\nscore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train_svd, y_train)\npreds = model.predict(X_test_svd)\nscore = accuracy_score(preds, y_test)\nscore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(max_iter=1000)  # You can choose other kernels like 'rbf', 'poly', etc.\nmodel.fit(X_train_svd, y_train)\npreds = model.predict(X_test_svd)\n\nscore = accuracy_score(preds, y_test)\nscore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X_train_svd, label=y_train)\ndtest = xgb.DMatrix(X_test_svd, label=y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n#class_weight = [1.23506253, 1.37661332, 0.49729579, 2.04486196, 0.571597, 4.66171329]\nclasses = [0, 1, 2, 3, 4, 5]\n    \nclass_weights = {l:w for w,l in zip(class_weight, classes)}\n\nxgb_clf = xgb.XGBClassifier(objective='multi:softmax',\n                            learning_rate=0.5,\n                            num_class=6, \n                            n_estimators=200,\n                            early_stopping_rounds=10, \n                            eval_metric=['merror','mlogloss'], \n                            seed=42)\nxgb_clf.fit(X_train_vec, \n            y_train,\n            #sample_weight=[class_weights[y] for y in y_train],\n            verbose=1, # set to 1 to see xgb training round intermediate results\n            eval_set=[(X_train_vec, y_train), (X_test_vec, y_test)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = xgb_clf.predict(X_val_vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(preds, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_prob = bst.predict(dtest)\ny_pred = [1 if pred > 0.5 else 0 for pred in y_pred_prob]\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 91.54% TF-idf-ngram range (1, 2)/XGBoost-0.5 lr, 200 estimators.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = lambda x:len(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n# Assuming texts is a list of preprocessed text strings\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n\n# Tokenize the texts\nsequences = tokenizer.texts_to_sequences(X_train)\nword_index = tokenizer.word_index\n\nprint(len(sequences))\n\nmax_length = train_df['text'].apply(lambda x: len(x.split())).max() + 1\n\n# Pad the sequences\npadded_sequences = pad_sequences(sequences, maxlen=max_length)\n\n# Define the model\nvocab_size = len(word_index) + 1\n\npadded_sequences[:2]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:29:57.588559Z","iopub.execute_input":"2024-07-14T19:29:57.589868Z","iopub.status.idle":"2024-07-14T19:29:58.584484Z","shell.execute_reply.started":"2024-07-14T19:29:57.589792Z","shell.execute_reply":"2024-07-14T19:29:58.582894Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"15999\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    1,   39,  100,   59,    7,   14,  493,    4,   14,\n        3495,  552,   31,   59,   60,  127,  147,   75, 1479,    3,   21,\n        1254],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,   16, 3059,    6, 1148,    4,  285,    1,    2,  494,\n         437],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    1,   23,  164,    7,  664,   26,\n           5, 4157,    1,   58,   46,    8,   12,   21,   71,   29,    5,\n        3496],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    1,   23,    7,\n        1064],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          72,   47,    7,    6,   55,  520,  318,  327,  157,  160,    8,\n          19]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"max_length","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:30:01.070542Z","iopub.execute_input":"2024-07-14T19:30:01.071178Z","iopub.status.idle":"2024-07-14T19:30:01.090582Z","shell.execute_reply.started":"2024-07-14T19:30:01.071132Z","shell.execute_reply":"2024-07-14T19:30:01.083851Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"67"},"metadata":{}}]},{"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(X_test)\nword_index = tokenizer.word_index\n\nprint(len(sequences))\n\n# Pad the sequences\ntest_padded_sequences = pad_sequences(sequences, maxlen=max_length)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:30:05.175947Z","iopub.execute_input":"2024-07-14T19:30:05.177164Z","iopub.status.idle":"2024-07-14T19:30:05.248376Z","shell.execute_reply.started":"2024-07-14T19:30:05.177125Z","shell.execute_reply":"2024-07-14T19:30:05.246924Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"1999\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TextLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers=1, bidirectional=False, dropout=0.5):\n        super(TextLSTM, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n                            bidirectional=bidirectional, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        embedded = self.dropout(self.embedding(x))\n        lstm_out, (hidden, cell) = self.lstm(embedded)\n        \n        if self.lstm.bidirectional:\n            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n        else:\n            hidden = self.dropout(hidden[-1,:,:])\n        \n        output = self.fc(hidden)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:30:05.462897Z","iopub.execute_input":"2024-07-14T19:30:05.463305Z","iopub.status.idle":"2024-07-14T19:30:05.475298Z","shell.execute_reply.started":"2024-07-14T19:30:05.463274Z","shell.execute_reply":"2024-07-14T19:30:05.473756Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nX_train_tensor = torch.tensor(padded_sequences)\ny_train_tensor = torch.tensor(y_train)\n# Assuming X_train_tensor and y_train_tensor are your prepared tensors\n\nbatch_size = 64\n\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nX_test_tensor = torch.tensor(test_padded_sequences)\ny_test_tensor = torch.tensor(y_test)\n# Assuming X_train_tensor and y_train_tensor are your prepared tensors\n\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\n\n# Model hyperparameters\nembedding_dim = 100\nhidden_dim = 256\noutput_dim = len(set(y_train))  # Number of classes\nn_layers = 2\nbidirectional = True\ndropout = 0.5\n\n# Instantiate the model, loss function, and optimizer\nmodel = TextLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:30:07.642338Z","iopub.execute_input":"2024-07-14T19:30:07.642755Z","iopub.status.idle":"2024-07-14T19:30:07.809259Z","shell.execute_reply.started":"2024-07-14T19:30:07.642721Z","shell.execute_reply":"2024-07-14T19:30:07.808030Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def accuracy_score(loader, model):\n    correct = 0\n    total = 0\n    model.eval()\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            predictions = model(X_batch)\n            _, predicted = torch.max(predictions.data, 1)\n            total += y_batch.size(0)\n            correct += (predicted == y_batch).sum().item()\n    accuracy = 100 * correct / total\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:30:08.683216Z","iopub.execute_input":"2024-07-14T19:30:08.683608Z","iopub.status.idle":"2024-07-14T19:30:08.691335Z","shell.execute_reply.started":"2024-07-14T19:30:08.683579Z","shell.execute_reply":"2024-07-14T19:30:08.689851Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Training loop\nepochs = 10\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    model.train()\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        \n        # Forward pass\n        predictions = model(X_batch)\n        \n        # Calculate loss\n        loss = criterion(predictions, y_batch)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f'Epoch {epoch+1}/{epochs},train loss: {avg_loss:.4f}')\n    \n    accuracy = accuracy_score(test_loader, model)\n    print(f'Accuracy: {(accuracy*100):.4f}%')    \n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:30:29.867791Z","iopub.execute_input":"2024-07-14T19:30:29.868200Z","iopub.status.idle":"2024-07-14T20:09:29.194691Z","shell.execute_reply.started":"2024-07-14T19:30:29.868173Z","shell.execute_reply":"2024-07-14T20:09:29.193425Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Epoch 1/10,train loss: 1.5648\nAccuracy: 4352.1761%\nEpoch 2/10,train loss: 1.4553\nAccuracy: 5817.9090%\nEpoch 3/10,train loss: 1.1763\nAccuracy: 7688.8444%\nEpoch 4/10,train loss: 0.8348\nAccuracy: 8499.2496%\nEpoch 5/10,train loss: 0.6082\nAccuracy: 8939.4697%\nEpoch 6/10,train loss: 0.4550\nAccuracy: 9159.5798%\nEpoch 7/10,train loss: 0.3808\nAccuracy: 9154.5773%\nEpoch 8/10,train loss: 0.3124\nAccuracy: 9244.6223%\nEpoch 9/10,train loss: 0.2792\nAccuracy: 9269.6348%\nEpoch 10/10,train loss: 0.2502\nAccuracy: 9249.6248%\n","output_type":"stream"}]},{"cell_type":"code","source":"accuracy = accuracy_score(val_loader, model)\nprint(f'Accuracy: {(accuracy*100):.4f}%')  ","metadata":{"execution":{"iopub.status.busy":"2024-07-14T20:09:29.197023Z","iopub.execute_input":"2024-07-14T20:09:29.197777Z","iopub.status.idle":"2024-07-14T20:09:29.251212Z","shell.execute_reply.started":"2024-07-14T20:09:29.197734Z","shell.execute_reply":"2024-07-14T20:09:29.249598Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43mval_loader\u001b[49m, model)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n","\u001b[0;31mNameError\u001b[0m: name 'val_loader' is not defined"],"ename":"NameError","evalue":"name 'val_loader' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# 9264.6323% LSTM 2 hidden layers 100 embedding dims, lemmatized text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}